自动语音识别（Automatic Speech Recognition，ASR）技术的实现过程主要分为两段。一个是生成机器能理解的声音向量。第二个是通过模型算法识别这些声音向量，最终给出识别结果。

![](https://github.com/guohui15661353950/pictures/blob/master/屏幕快照%202019-03-19%20上午10.35.14.png?raw=true)

####一. 语音识别系统的组成结构

主要分4部分：
信号处理和特征提取、声学模型（AM）、语言模型（LM）和解码搜索部分。

* 信号处理和特征提取：
  以音频信号为输入，通过消除噪声和信道失真对语音进行增强，将信号从时域转化到频域，并为后面的声学模型提取合适的有代表性的特征向量。
* 声学模型：
  将声学和发音学的知识进行整合，以特征提取部分生成的特征为输入，并为可变长特征序列生成声学模型分数。

* 语言模型：
  语言模型估计通过训练语料学习词与词之间的相互关系，来估计假设词序列的可能性，又叫语言模型分数。如果了解领域或任务相关的先验知识，语言模型的分数通常可以估计的更准确。
* 解码搜索：
  综合声学模型分数与语言模型分数的结果，将总体输出分数最高的词序列当做识别结果。



####二. 信号的数字化和预处理

要将收集到的语音转化为一系列的数值，这样机器才可以理解。

* 数字化

  声音是作为波的形式传播的。将声波转换成数字的步骤为：模拟信号➡️采样➡️量化➡️数字信号

  为了将声波转换成数字，我们只记录声波在等距点的高度，这被称为采样（sampling）。采样定理（Nyquist theorem）规定，从间隔的采样中完美重建原始声波——只要我们的采样频率比期望得到的最高频率快至少两倍就行。

  经过采样，我们获取了一系列的数字，这些数字才可以在机器上进行建模或计算。

  我们每秒读取数千次，并把声波在该时间点的高度用一个数字记录下来。把每一秒钟所采样的数目称为采样频率或采率，单位为HZ（赫兹）。

* 采样信号预处理

  这里的预处理主要指分帧处理，因为语音信号是不平稳的、时长变化的，我们把它分隔为一小段一小段（10毫秒-40毫秒）的短语音，我们认为这样的小片段是平稳的，称之为【帧】。

  在每个帧上进行信号分析，称为语音的短时分析。

  图中，每帧的长度为25毫秒，每两帧之间有25-10=15毫秒的交叠。我们称为帧长25ms、帧移10ms的分帧。

  帧移是为了保证语音信息的完整性。感兴趣的同学可以查一下，加窗/窗函数。

  那为什么需要平缓的分帧呢？因为我们需要做傅里叶变化，它适用于分析平稳的信号。（想弄明白傅里叶变换的，之后可以参考文章末尾的链接）

  人类是根据振动频率判断声音的，而以时间为横轴（时域）的波形图没有振幅描述，我们需要做傅里叶变换，将它变成以频率为横轴（频域）的振幅描述。



####三. 特征提取

特征提取就是从语音波形中提取出能反映语音特征的重要信息，去掉相对无关的信息（如背景噪声），并把这些信息转换为一组离散的参数矢量 。

* 特征提取

  用傅里叶变化来完成时域到频域的转换。这就需要对每一帧做傅里叶变化，用特征参数MFCC得到每一帧的频谱（这个过程就是特征提取，结果用多维向量表示），最后可以总结为一个频谱图（语谱图）。

* 常用的特征参数

  特性提取时，我们有常用的特征参数作为提取模板，主要有两种：

  - 线性预测系数（LPC）

    LPC 的基本思想是，当前时刻的信号可以用若干个历史时刻的信号的线性组合来估计。通过使实际语音的采样值和线性预测采样值之间达到均方差最小，即可得到一组线性预测系数。

    求解LPC系数可以采用自相关法 (德宾 durbin 法) 、协方差法、格型法等快速算法。

  - 倒谱系数

    利用同态处理方法，对语音信号求离散傅立叶变换后取对数，再求反变换就可得到倒谱系数。

    其中，LPC倒谱(LPCCEP)是建立在LPC谱上的。而梅尔倒谱系数（Mel Frequency Cepstrum Coefficient, MFCC）则是基于MEL谱的。不同于LPC等通过对人的发声机理的研究而得到的声学特征，MFCC 是受人的听觉系统研究成果推动而导出的声学特征。

    简单的说，经过梅尔倒谱分析，得到的参数更符合人耳的听觉特性。



#### 四. 声学模型

声学模型是识别系统的底层模型，其目的是提供一种计算语音的特征矢量序列和每个发音模板之间的距离的方法。

也就是说，提取到的语音特性，与某个发音之间的差距越小，越有可能是这个发音。

或者说，某帧对应哪个状态的概率最大，那这帧就属于哪个状态。这个可以用GMM（混合高斯模型，就是一种概率分布）或DNN（深度神经网络）来识别。

但这样识别出来的结果会比较乱，因为一个人讲话的速度不一样。这个问题可以用DTW（动态时间规整）或HMM（隐马尔科夫模型）或CTC（改进的RNN模型）来对齐识别结果，知道单词从哪里开始，从哪里结束，哪些内容是重复的没有必要的。

#####1）常用的声学建模方法包含以下三种：

- 基于模式匹配的动态时间规整法(DTW)；
- 隐马尔可夫模型法(HMM)；
- 基于人工神经网络识别法(ANN)；

在过去，主流的语音识别系统通常使用梅尔倒谱系数（Mel-Frequency Cepstral Coefficient, MFCC）或者线性感知预测（Perceptual Linear Prediction, PLP）作为特征，使用混合高斯模型-隐马尔科夫模型（GMM-HMM）作为声学模型。

近些年，分层鉴别模型比如DNN，变得可行起来，比如上下文相关的深度神经网络-隐马尔可夫模型（context-dependent DNN-HMM，CD-DNN-HMM）就比传统的GMM-HMM表现要好得多。

##### 2）主要问题：

我们要了解的是，声学模型存在2个问题：

* 特征向量序列的可变长；
  每个人说同一个单词的时间长度都不一样，声学模型要能从不同的时间长度的语音信号中识别出是同一个单词。

  解决方法就是DTW（动态时间规整）、 HMM（隐马尔可夫模型）。

* 音频信号的丰富变化性；
  如说话人的性别，健康状况，紧张程度，说话风格、语速，环境噪音，周围人声，信道扭曲，方言差异，非母语口音等。

#####3）HMM 声学建模：

对语音识别系统而言，HMM 的输出值通常就是各个帧的声学特征 。 为了降低模型的复杂度，通常 HMM 模型有两个假设前提，一是内部状态的转移只与上一状态有关，一是输出值只与当前状态或当前状态转移有关。除了这两个假设外，HMM 模型还存在着一些理论上的假设，其中之一就是，它假设语音是一个严格的马尔科夫过程 。





